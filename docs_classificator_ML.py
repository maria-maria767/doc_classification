# -*- coding: utf-8 -*-
"""Docs_Classificator_P2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Yob1B1Oq3c42HTE4F8QAI-MJ8CRJqtzq
"""

# Commented out IPython magic to ensure Python compatibility.
# Импорт необходимых библиотек для монтирования диска
import os

#==монтирование google drive в colab и просмотр содержимого dataset
from google.colab import drive
drive.mount('/content/drive')
# %cd /content/drive/My Drive/Data/docs_ds/

"""**1. Настройка**

***1.1 Импорт и конфигурирование библиотек***
"""

# Библиотеки для обработки данных.
import numpy as np
import scipy as sp
from scipy.spatial import distance
import pandas as pd

# Библиотеки для визуализации данных.
import matplotlib
import matplotlib.pyplot as plt
import seaborn as sns
from prettytable import PrettyTable


# Библиотеки для моделирования данных.
import sklearn
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.manifold import TSNE
from sklearn.metrics import f1_score
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras import utils, Input, callbacks
from tensorflow.keras.layers import Conv1D, Dense, Embedding, Flatten, MaxPool1D, Dropout

# Служебные библиотеки.
import re
import math
import time
import shutil


# Определение версий библиотек.
print("Версия NumPy:", np.__version__)
print("Версия Pandas:", pd.__version__)
print("Версия Matplotlib:", matplotlib.__version__)
print("Версия Seaborn:", sns.__version__)
print("Версия Scikit-learn:", sklearn.__version__)
print("Версия TensorFlow:", tf.__version__)

# Настройка NumPy.
# Установка точности до четырёх знаков после запятой.
# Установка `Ширины строки` на значение `Максимум 130 символов в выводе.
# После вывода 130 символов, вывод продолжится на следующей строке.
np.set_printoptions(precision=4, linewidth=130)

# Конфигурация Seaborn.
sns.set_style("whitegrid")  # Установка фона белого цвета и сетки.
sns.set_palette("deep")  # Установка цветовой палитры.
sns.set_context("paper", font_scale=1.25)  # Установка размера шрифта на 1.25 больше обычного.

# Конфигурация Tensorflow.
tf.random.set_seed(100)
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "1"

"""***1.2 Задание констант и вспомогательных (служебных) функций***"""

# Константы.
DOCS_CSV = "/content/drive/My Drive/Data/preprocessed/preprocessed.csv"
GLOVE_TXT = "/content/drive/My Drive/Data/glove/glove.6B.100d.txt"

LOGS_DIR = "/content/drive/My Drive/Data/logs"
MODELS_DIR = "/content/drive/My Drive/Data/models"
MODEL_FILE = "/content/drive/My Drive/Data/models/epoch-{epoch:02d}_accuracy-{val_accuracy:.4f}.keras"


TYPES_COUNT = 4  # Количество меток типа (класса).
OUTPUT_DIM = 100  # Размерность выходного измерения.

# Предел Epochs для раннего завершения обучения (EarlyStopping).
# Количество эпох без улучшений, после которых обучение будет остановлено.
PATIENCE = 3

# Сопоставление классов и меток
types_map = {'Confidential': 0, 'Internal': 1, 'Public': 2, 'Restricted': 3}

# Вспомогательные (служебные) функции.

#===Функция сжатия строки путем отображения многоточия между
#   первыми n и последними n символами
def func_viewNValues(row_loc, n=5):
    if row_loc is None:
        return "[]"

    left_hand_symbols = []
    right_hand_symbols = []
    for idx, value in enumerate(row_loc):
        if idx < n:
            left_hand_symbols.append(str(value))
        elif idx >= (len(row_loc) - n):
            right_hand_symbols.append(str(value))

    left_hand_symbols = ", ".join(left_hand_symbols)
    delimeters = ", ..." if len(row_loc) > 2 * n else ""
    right_hand_symbols = f", {', '.join(right_hand_symbols)}" if len(right_hand_symbols) else ""

    return f"[{left_hand_symbols + delimeters + right_hand_symbols}]"

# Lambda-функция, уменьшающая входное 'value' на 'percent' процентов.
decreaseBy = lambda value, percent: value - (percent * value / 100)


#===Функция вычисления косинусного сходства между двумя входными словами
#   на основе их GloVe-векторов
def func_cosineSimilarity(glove_vectors_index_loc, word_1_loc, word_2_loc):

    vector_1 = glove_vectors_index_loc.get(word_1_loc.lower())
    print(f"Вектор GloVe для `{word_1_loc}`: {func_viewNValues(vector_1, 3)}")

    vector_2 = glove_vectors_index_loc.get(word_2_loc.lower())
    print(f"Вектор GloVe для `{word_2_loc}`: {func_viewNValues(vector_2, 3)}")

    cosine_distance = distance.cosine(vector_1, vector_2)
    print(f"Косинусное сходство между '{word_1_loc}' и '{word_2_loc}': {round(cosine_distance, 4)}")

"""***1.3 Сброс содержимого каталогов***"""

# Удаление журналов и сохраненных моделей.
shutil.rmtree(LOGS_DIR, ignore_errors=True)
shutil.rmtree(MODELS_DIR, ignore_errors=True)

"""***1.4 Загрузка набора данных***"""

documents_dataframe = pd.read_csv(DOCS_CSV)
documents_dataframe.head(3)

"""**2. EDA (анализ данных)**"""

rows, columns = documents_dataframe.shape
print(f"Набор данных содержит {rows} строк и {columns} столбцов.\n")

documents_dataframe.info()

"""**3. Служебные операции**

***3.1 Токенизация***
"""

documents_tokenizer = Tokenizer(
    num_words=5000,
    filters='!"#$%&()*+,-./:;<=>?@[\\]^`{|}~\t\n',  # Удаление символа подчеркивания.
    oov_token="<UNK>",  # Токен, отсутствующий в словаре.
)

documents_tokenizer.fit_on_texts(documents_dataframe["Текст"].values)

# Индексация слов.
word_index = documents_tokenizer.word_index
vocabulary_size = len(word_index)  # Размер словаря.

print("Общее количество слов в словаре:", vocabulary_size)
print("\nПример индексации слов:")
{k: v for k, v in list(word_index.items())[:10]}

# Кодированиетекста документов в последовательности.
encoding_documents = documents_tokenizer.texts_to_sequences(documents_dataframe["Текст"].values)

print("Последовательности:")
print("Тип данных:", type(encoding_documents))
print("Общее количество:", len(encoding_documents))
print("Примеры значений:")
_ = [print(func_viewNValues(row_curr)) for row_curr in encoding_documents[:3]]

sequence_lengths = [len(x) for x in encoding_documents]

# Получение максимальной длины обучающей последовательности.
maximum_length = max(sequence_lengths)
print("Размер самой длинной последовательности:", maximum_length)

# Получение минимальной длины обучающей последовательности.
minimum_length = min(sequence_lengths)
print("Размер самой короткой последовательности:", minimum_length)

# Определение наилучшего значения длины последовательности.
plt.figure(figsize=(9, 5))

sns.histplot(x=sequence_lengths)
plt.xlabel("Длина последовательности")
plt.ylabel("Количество")
plt.title("Распределение длин последовательностей")

plt.show()

# Выбор длины последовательности в соответствии с методом Elbow
low_level, high_level = 0.95, 1
plt.figure(figsize=(6, 5))

x = np.arange(low_level, high_level, 0.01)
y = np.quantile(a=np.array(sequence_lengths), q=x)

sns.lineplot(x=x, y=y)
plt.title(f"Длина последовательности между {low_level} & {round(high_level - 0.01, 2)} процентилями")
plt.xlabel("Процентиль")
plt.ylabel("Длина последовательности")

plt.show()

# ~99% документов имеют длину последовательности менее 380, поэтому выбирается это значение для sequence_length.
sequence_length = 380

"""***3.2 Дополнение (Padding)***"""

padding_documents = pad_sequences(encoding_documents, maxlen=sequence_length, padding="post", truncating="post")

print("Дополненные последовательности:")
print("Тип данных:", type(padding_documents), "Размерность:", padding_documents.shape)
print("Примеры значений:")
print(padding_documents)

"""***3.3 Векторы GloVe***"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# # Загрузка векторов слов.
# # Загрузка полного набора.
# glove_vectors_index = dict()
# with open(GLOVE_TXT, encoding="utf8") as glove_text_file:
#     for line in glove_text_file.readlines():
#         columns = line.split()
#         word_current = columns[0]
#         vec = np.array(columns[1:], dtype="float32")
#         glove_vectors_index[word_current] = vec

print("Общее количество слов в GloVe:", len(glove_vectors_index.keys()))
print("Примеры векторов слов:")

result_table = PrettyTable(["Слово", "Предварительно обученный вектор"], align="l")
result_table.add_rows([[k, func_viewNValues(v, n=3)] for k, v in list(glove_vectors_index.items())[:5]])

print(result_table)

# Пропущенные слова.
missing_words = [word_current for word_current in word_index.keys() if word_current not in glove_vectors_index.keys()]
missing_words_count = len(missing_words)
missing_words_percent = round(missing_words_count / len(word_index.keys()) * 100, 1)
print(f"{missing_words_count} то есть, {missing_words_percent}% слов в словаре не имеют векторов GloVe.")

# Загрузка весов.

# Все нулевые векторы.
zero_vectors = np.zeros(OUTPUT_DIM)

weights = [zero_vectors]  # Для <PAD>

# Создание матрицы весов для слов в словаре.
for word_current in word_index.keys():
    weights.append(glove_vectors_index.get(word_current, zero_vectors).tolist())


weights = tf.convert_to_tensor(weights, dtype=tf.float32)
print("Тип данных весов:", weights.dtype)
print("Размерность весов:", weights.shape)

"""***3.4 Косинусное подобие***"""

# Пример 1
func_cosineSimilarity(glove_vectors_index, "Color", "Colour")

func_cosineSimilarity(glove_vectors_index, "Juice", "Table")

"""***3.5 Функции POS***"""

# Количество частей речи в документе.
# Этот признак целесообразно включить вместе с дополненными последовательностями
# в качестве входных данных для сети.

documents_dataframe.iloc[:, 3:18].astype("int32").head()

"""**4. Настройка пользовательских Callbacks**

***4.1 Callback #1: Показатели производительности***¶
"""

# Callback для вычисления и печати оценки F1 для набора проверочных данных.
class ProductivityIndicators(callbacks.Callback):
    def __init__(self, X_test, y_test):
        self.X_test = X_test
        self.y_test = y_test

    def on_epoch_end(self, epoch, logs=None):
        if self.model.stop_training:
            # Остановка распространения Callbacks.
            return

        cl_probability = self.model.predict(self.X_test)
        y_prediction = np.argmax(cl_probability, axis=1)

        f1_scr = f1_score(self.y_test, y_prediction, average="micro")
        print("F1-score для проверочного набора данных:", round(f1_scr, 4))

"""***4.2 Callback #2. SaveBetterModel***"""

# Callback для сохранения модели в каждой эпохе, если точность проверки
# улучшается по сравнению с предыдущей эпохой.

class SaveBetterModel(callbacks.ModelCheckpoint):
    def __init__(self, **kwargs):
        super().__init__(
            monitor="val_accuracy",
            verbose=2,
            save_best_only=True,
            mode="max",
            **kwargs,
        )

    def on_epoch_end(self, epoch, logs=None):
        # Для остановки распространения Callbacks.
        if self.model.stop_training:
            return

        super().on_epoch_end(epoch, logs)

"""***4.3 Callback #3. TerminateWhenNaN***"""

class TerminateWhenNaN(callbacks.Callback):
    def on_epoch_end(self, epoch, logs=None):
        if self.model.stop_training:
            # Остановка распространения Callbacks.
            return

        # Проверка уровня потерь для значений NaN.
        loss = logs.get("loss")
        if (loss is not None) and (np.isnan(loss) or np.isinf(loss)):
            print("Недействительная потеря. Жпоха прекращена:", epoch)
            # Сообщить следующему Callback о пропуске выполнения
            self.model.stop_training = True
            return

"""***4.4 Callback #4. DecayLearningRate***"""

# Уменьшение скорости обучения на 10%, если точность проверки текущей эпохи ниже точности предыдущей эпохи.
# Уменьшение скорости обучения на 5% каждую третью эпоху.

class DecayLearningRate(callbacks.Callback):
    def __init__(self):
        self.prev_val_acc = 0  # previous validation accuracy.

    def __get_lr__(self):
        return float(tf.keras.backend.get_value(self.model.optimizer.learning_rate))

    def __set_lr__(self, new_lr):
        # tf.keras.backend.set_value(self.model.optimizer.learning_rate, new_lr)
        new_lr = np.asarray(new_lr, dtype=x.dtype)
        self.model.optimizer.learning_rate.assign(new_lr)

    def on_epoch_end(self, epoch, logs=None):
        if self.model.stop_training:
            # Stop propagation of callbacks.
            return

        curr_val_acc = logs.get("val_accuracy")
        lr = self.__get_lr__()
        if curr_val_acc < self.prev_val_acc:
            new_lr = decreaseBy(lr, 10)
            self.__set_lr__(new_lr)
            print("Current epoch validation accuracy:", round(curr_val_acc, 4))
            print("Previous epoch validation accuracy:", round(self.prev_val_acc, 4))
            print("Reducing learning rate by 10%. New learning rate:", round(new_lr, 6))
        elif not (epoch + 1) % 3:
            # Since epochs start at zero, add one to get current epoch number.
            new_lr = decreaseBy(lr, 5)
            self.__set_lr__(new_lr)
            print("Reducing learning rate by 5%. New learning rate:", round(new_lr, 6))

        # Save current validation accuracy value for next epoch.
        self.prev_val_acc = curr_val_acc

"""***4.5 Callback #5. TerminayeOnNoChange***"""

# Ранняя остановка: обучение прекращается, когда точность проверки перестает улучшаться.

class TerminateOnNoChange(callbacks.EarlyStopping):
    def __init__(self, epochs):
        super().__init__(monitor="val_accuracy", patience=epochs, verbose=2, mode="max")

    def on_epoch_end(self, epoch, logs=None):
        if self.model.stop_training:
            # Остановка распространения Callbacks.
            return

        super().on_epoch_end(epoch, logs)

"""**5. Настройка обучения**

***5.1 Разделение выборки на обучающую, тестовую и валидационную (проверочную)***
"""

X = np.column_stack((padding_documents, documents_dataframe.iloc[:, 3:18].astype("int32")))
y = documents_dataframe["Категория"]

X_train, X_test, y_train, y_test = train_test_split(
    X,
    y,
    test_size=0.25,
    stratify=y,
    random_state=42,
)

X_train, X_cval, y_train, y_cval = train_test_split(
    X_train,
    y_train,
    test_size=0.3,
    stratify=y_train,
    random_state=42,
)

print("X обучающий набор данных:", X_train.shape)
print("y обучающий набор данных:", y_train.shape)

print("X набор данных кросс-валидации:", X_cval.shape)
print("y набор данных кросс-валидации:", y_cval.shape)

print("X тестовый набор данных:", X_test.shape)
print("y тестовый набор данных:", y_test.shape)

"""***5.2 Масштабирование***"""

documents_scaler = StandardScaler()
documents_scaler.fit(X_train)

scaled_X_train = documents_scaler.transform(X_train)  # Масштабирование X_train.
scaled_X_cval = documents_scaler.transform(X_cval)  # Масштабирование X_cval.
scaled_X_test = documents_scaler.transform(X_test)  # Масштабирование X_test.

print("Масштабированые значения X train (случайный пример):")
print(np.round(scaled_X_train[200:203], 3))

print("\nМасштабированые значения X cross-val (случайный пример):")
print(np.round(scaled_X_cval[115:118], 3))

print("\nМасштабированые значения X test (случайный пример):")
print(np.round(scaled_X_test[274:277], 3))

"""Примечание: Масштабирование не использовалось, поскольку оно не показало увеличения точности свыше 35%.

***5.3 Изменение размерности обучающих данных***
"""

# Расширение размерности наборов данных.

# Примечание: Для включения масштабирования необходимо заменить
# X_train, X_cval и X_test в коде ниже на scaled_X_train, scaled_X_cval и scaled_X_test.

expand_X_train = np.expand_dims(X_train, axis=2)  # Расширенный X_train.
expand_X_cval = np.expand_dims(X_cval, axis=2)  # Расширенный X_cval.
expand_X_test = np.expand_dims(X_test, axis=2)  # Расширенный X_test.

print("Расширенный обучающий набор данных X train:", expand_X_train.shape)
print("Расширенный набор данных кросс-валидации:", expand_X_cval.shape)
print("Расширенный тестовый набор данных:", expand_X_test.shape)

"""**6. Работа с моделью (моделирование)**"""

# Сброс всех состояний, сгенерированных Keras.
tf.keras.backend.clear_session()

"""***6.1 Определение архитектуры модели***"""

rows, cols = X_train.shape

# ВХодной слой
inputs = Input(shape=(cols,), name="Input_Documents_Text")

# Embedding слой.
x = Embedding(
    vocabulary_size + 1,
    OUTPUT_DIM,
    input_length=cols,
    name="EmbeddingLayer1",
    weights=[weights],
    trainable=False,
)(inputs)

# Три параллельных конволючионных слоя с различным размером фильтра + Max pooling.
f1 = Conv1D(4, 3, padding="same", activation="relu", name="Conv1D_F1_Filters")(x)
f2 = Conv1D(8, 3, padding="same", activation="relu", name="Conv1D_F2_Filters")(x)
f3 = Conv1D(16, 3, padding="same", activation="relu", name="Conv1D_F3_Filters")(x)
f1f2f3 = tf.keras.layers.concatenate([f1, f2, f3], name="Concatenate1_F1F2F3")
x = MaxPool1D(3, name="MaxPoolLayer1")(f1f2f3)

# Три параллельных конволючионных слоя с различным размером фильтра + Max pooling.
f4 = Conv1D(2, 3, padding="same", activation="relu", name="Conv1D_F4_Filters")(x)
f5 = Conv1D(4, 3, padding="same", activation="relu", name="Conv1D_F5_Filters")(x)
f6 = Conv1D(8, 3, padding="same", activation="relu", name="Conv1D_F6_Filters")(x)
f4f5f6 = tf.keras.layers.concatenate([f4, f5, f6], name="Concatenate2_F4F5F6")
x = MaxPool1D(3, name="MaxPoolLayer2")(f4f5f6)

x = Conv1D(8, 3, padding="same", activation="relu", name="Conv1D_P_Filters")(x)
x = Flatten(name="Flatten")(x)
x = Dropout(0.25, name="DropOut")(x)
x = Dense(30, activation="relu", name="Dense1")(x)

outputs = Dense(TYPES_COUNT, activation="softmax", name="Output_layer")(x)

work_model = tf.keras.Model(inputs=inputs, outputs=outputs, name="Documents_Classification")
work_model.summary()

"""***6.2 Отображение архитектуры модели***"""

utils.plot_model(work_model, "Documents_Classification_Model.png", show_shapes=True)

"""***6.3 Компиляция модели***"""

work_model.compile(
    loss="sparse_categorical_crossentropy",
    optimizer=tf.keras.optimizers.Adam(0.002),
    metrics=["accuracy"],
)

"""***6.4 Обучение модели***"""

# Обработка несбалансированного набора данных
types_weights = {}
for types, count in y_train.value_counts().sort_index().items():
    types_weights[types] = round(1 - (count / len(y_train)), 2)  # Вариант 1
    # types_weights[cls] = (1 / count) * (len(y_train) / 2.0)  # Вариант 2

print("Веса типов документов:", types_weights)

# Запуск обучения модели
train_history = work_model.fit(
    x=expand_X_train,
    y=y_train,
    batch_size=89,
    epochs=30,
    verbose=2,
    callbacks=[
        callbacks.TensorBoard(LOGS_DIR, histogram_freq=1),
        TerminateWhenNaN(),
        ProductivityIndicators(expand_X_cval, y_cval),
        DecayLearningRate(),
        SaveBetterModel(filepath=MODEL_FILE),
        TerminateOnNoChange(epochs=PATIENCE),
    ],
    validation_data=(expand_X_cval, y_cval),
    # class_weight=cls_weights,
)

"""***6.5 Статистика обучения модели***"""

#===Функция отрисовки графиков обучения модели
def func_drawGraphics(history_loc, metric_loc):
    plt.plot(history_loc.history[metric_loc])
    plt.plot(history_loc.history['val_'+metric_loc], '')
    plt.xlabel("Эпохи")
    plt.ylabel(metric_loc)
    plt.legend([metric_loc, 'val_'+metric_loc])

plt.figure(figsize=(16, 8), dpi=300)
plt.subplot(1, 2, 1)
func_drawGraphics(train_history, 'accuracy')
plt.ylim(None, 1)
plt.subplot(1, 2, 2)
func_drawGraphics(train_history, 'loss')
plt.ylim(0, None)

plt.show

"""***6.6 Проверка модели***"""

work_model.evaluate(expand_X_test, y_test, verbose=2)

"""**7. Показатели производительности**

***7.1 Матрица путаницы***
"""

# Предсказание меток класса y_test
types_probability = work_model.predict(expand_X_test)
y_pred = np.argmax(types_probability, axis=1)

# Вычисление матрицы путаницы
confusion_matrix = tf.math.confusion_matrix(y_test, y_pred, TYPES_COUNT).numpy()

# Вывод матрицы путаницы
plt.figure(figsize=(7, 6))
sns.heatmap(
    confusion_matrix,
    annot=True,
    fmt="d",
    cmap="YlGnBu",
    cbar=False,
    linecolor="white",
    linewidths=1,
    yticklabels=types_map.keys(),
)
plt.title("Матрица путаницы")
plt.xlabel("Предсказанные значения")
plt.ylabel("Истинные значения")

plt.show()

new_data = [02]
preds = work_model(new_data, return_all_scores=True)
preds